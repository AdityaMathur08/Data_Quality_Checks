{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5d2c27c4-4e91-4924-aefe-2afa5bc5e33b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Requirement already satisfied: pretty_html_table in /databricks/python3/lib/python3.8/site-packages (0.9.16)\n","Requirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from pretty_html_table) (1.2.4)\n","Requirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (2020.5)\n","Requirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (1.20.1)\n","Requirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (2.8.1)\n","Requirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;pretty_html_table) (1.15.0)\n","WARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\n","You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n","Requirement already satisfied: openpyxl in /databricks/python3/lib/python3.8/site-packages (3.0.10)\n","Requirement already satisfied: et-xmlfile in /databricks/python3/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n","WARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\n","You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n","ERROR: Could not find a version that satisfies the requirement yaml\n","ERROR: No matching distribution found for yaml\n","WARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\n","You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n","fatal: destination path &#39;dateinfer&#39; already exists and is not an empty directory.\n","Processing /databricks/driver/dateinfer\n","Requirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from pydateinfer==0.4.1) (2020.5)\n","Building wheels for collected packages: pydateinfer\n","  Building wheel for pydateinfer (setup.py): started\n","  Building wheel for pydateinfer (setup.py): finished with status &#39;done&#39;\n","  Created wheel for pydateinfer: filename=pydateinfer-0.4.1-py3-none-any.whl size=14397 sha256=d3e9849fccb7cd42a390286c712de2c3938bf85feee79f1a05870f384a073840\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3w6rg0vm/wheels/b6/f8/d2/6efff5909de6298d820ba0b0e28c430f668df50cc40deacf39\n","Successfully built pydateinfer\n","Installing collected packages: pydateinfer\n","  Attempting uninstall: pydateinfer\n","    Found existing installation: pydateinfer 0.4.1\n","    Uninstalling pydateinfer-0.4.1:\n","      Successfully uninstalled pydateinfer-0.4.1\n","Successfully installed pydateinfer-0.4.1\n","WARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\n","You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Requirement already satisfied: pretty_html_table in /databricks/python3/lib/python3.8/site-packages (0.9.16)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from pretty_html_table) (1.2.4)\nRequirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (2020.5)\nRequirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (1.20.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;pretty_html_table) (2.8.1)\nRequirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;pretty_html_table) (1.15.0)\nWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\nRequirement already satisfied: openpyxl in /databricks/python3/lib/python3.8/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /databricks/python3/lib/python3.8/site-packages (from openpyxl) (1.1.0)\nWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\nERROR: Could not find a version that satisfies the requirement yaml\nERROR: No matching distribution found for yaml\nWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\nfatal: destination path &#39;dateinfer&#39; already exists and is not an empty directory.\nProcessing /databricks/driver/dateinfer\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from pydateinfer==0.4.1) (2020.5)\nBuilding wheels for collected packages: pydateinfer\n  Building wheel for pydateinfer (setup.py): started\n  Building wheel for pydateinfer (setup.py): finished with status &#39;done&#39;\n  Created wheel for pydateinfer: filename=pydateinfer-0.4.1-py3-none-any.whl size=14397 sha256=d3e9849fccb7cd42a390286c712de2c3938bf85feee79f1a05870f384a073840\n  Stored in directory: /tmp/pip-ephem-wheel-cache-3w6rg0vm/wheels/b6/f8/d2/6efff5909de6298d820ba0b0e28c430f668df50cc40deacf39\nSuccessfully built pydateinfer\nInstalling collected packages: pydateinfer\n  Attempting uninstall: pydateinfer\n    Found existing installation: pydateinfer 0.4.1\n    Uninstalling pydateinfer-0.4.1:\n      Successfully uninstalled pydateinfer-0.4.1\nSuccessfully installed pydateinfer-0.4.1\nWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["%sh\n","\n","pip install pretty_html_table\n","pip install openpyxl\n","\n","\n","git clone https://github.com/nedap/dateinfer.git\n","cd dateinfer\n","pip install ."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"79816a57-d364-475e-bbb8-0796df361708","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import pandas as pd\n","import numpy as np\n","import time\n","import dateinfer\n","import unicodedata\n","import re\n","from numpy import nansum\n","from numpy import nanmean\n","import pandas as pd\n","from shutil import copyfile\n","from openpyxl import Workbook\n","from openpyxl.utils.dataframe import dataframe_to_rows\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2e895ac0-e129-4f7e-b26e-404f8430a293","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def normalize(column: str) -> str:\n","    \"\"\"\n","    Normalize column name by replacing invalid characters with underscore\n","    strips accents and make lowercase\n","    :param column: column name\n","    :return: normalized column name\n","    \"\"\"\n","    n = re.sub(r\"[ ,;{}()\\n\\t=]+\", '_', column.lower())\n","    return unicodedata.normalize('NFKD', n).encode('ASCII', 'ignore').decode()\n","\n","def get_most_freq_date_fmt(date_col) -> pd.Series:\n","  \n","    date_col = date_col.apply(lambda x: dateinfer.infer([x]))\n","    date_col_freq_fmt = date_col.mode()[0]\n","    return date_col_freq_fmt  \n","  \n","def auto_date_parser(parse_df, date_cols, final_date_fmt):\n","    \n","    \n","  \n","  # Convert Date Formats of a column into most frequent date format\n","    infer_fmt = []\n","\n","    for column in date_cols:\n","    # Sample data and convert into Pandas\n","        pdf = parse_df.filter(parse_df[column].isNotNull()).limit(100)\n","        pdf = pdf.toPandas()\n","\n","        infer_fmt.append(get_most_freq_date_fmt(pdf[column]))\n","        print(column,\":\",infer_fmt)\n","        infer_fmt = list(set(infer_fmt))\n","        infer_fmt = [i.replace(\"%Y\",\"yyyy\").replace(\"%m\",\"MM\").replace(\"%d\",\"dd\")  for i in infer_fmt]\n","    # In sequence correct date format should be 1st.\n","        base_fmt = [\"yyyy-MM-dd\", \"yyyy MM dd\",\"yyyy MMMM dd\",\"%m/%d/%Y\",\"dd-MM-yyyy\", \"MM/dd/yyyy\",\"MM-dd-yyyy\", \"yyyy-MM-dd\", \"%d/%m/%Y\", \"dd/MM/yyyy\"]\n","        def to_date_(col, formats=infer_fmt + base_fmt):\n","      # Spark 2.2 or later syntax, for < 2.2 use unix_timestamp and cast\n","          print(\"formats:\",formats)\n","          return coalesce(*[to_date(col, f) for f in formats])\n","\n","        parse_df = parse_df.withColumn(column, date_format(to_date_(column), final_date_fmt))\n","    return parse_df\n","\n","def preprocess_data(df, numeric_cols):\n","    #, date_cols, final_dt_fmt = \"yyyy-MM-dd\"\n","    for Col in numeric_cols:\n","        \n","        df = df.withColumn(Col, regexp_replace(Col,'[^0-9.]',''))\n","        #df = df.withColumn(Col, regexp_replace(Col,'[\\,|\\ ]',''))\n","        #df = auto_date_parser(df, date_cols, final_date_fmt = final_dt_fmt)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5a1cf934-b18c-4fef-a273-2dd99f152ca0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def get_null_perc(spark, df, null_cols):\n","    \"\"\" Get null/empty percentage for columns\n","    Args:\n","        spark (Spark): SparkSession object\n","        df (DataFrame): dataframe to perform null/empty analysis on\n","        null_cols (List): list of columns that need to be considered for analysis \n","    Returns:\n","        DataFrame: dataframe with null check analysis\n","    \"\"\"\n","    schema = StructType([ \\\n","        StructField(\"Column\",StringType(),True), \\\n","        StructField(\"NullPercentage\",StringType(),True)\n","    ])\n","    emptyRDD = spark.sparkContext.emptyRDD()\n","    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n","\n","    for x in null_cols:\n","        if x.upper() in (name.upper() for name in df.columns):\n","            df_null_count = df.select(col(x)).filter(col(x).isNull() | (col(x) == '')).count()\n","            df_null = spark.createDataFrame([[x, f'{(df_null_count*100.0/df.count()):.2f}' + \"%\" ]],schema=schema)\n","            resultdf = resultdf.union(df_null)\n","    return resultdf\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0a8a71d5-cf28-45a2-9ce5-3e26994c1491","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def get_summary_numeric(df, numeric_cols):\n","    \"\"\" Get Summary for numeric columns\n","    Args:\n","        df (DataFrame): dataframe to perform analysis on\n","        numeric_cols (List): list of columns that need to be considered for analysis\n","    Returns:\n","        DataFrame: dataframe with summary analysis\n","    \"\"\"\n","    for x in numeric_cols:\n","        if x.upper() not in (name.upper() for name in df.columns):\n","            numeric_cols.remove(x)\n","    return df.select(numeric_cols).summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e340b47b-d4ce-4c8f-aa2e-19347f210366","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def get_distinct_counts(spark, df, aggregate_cols):\n","    \"\"\" Get distinct count for columns\n","    Args:\n","        spark (Spark): SparkSession object\n","        df (DataFrame): dataframe to perform distinct count analysis on\n","        aggregate_cols (List): list of columns that need to be considered for analysis\n","    Returns:\n","        DataFrame: dataframe with distinct count analysis\n","    \"\"\"\n","    schema = StructType([ \\\n","        StructField(\"Column\",StringType(),True), \\\n","        StructField(\"DistinctCount\",StringType(),True)\n","    ])\n","\n","    emptyRDD = spark.sparkContext.emptyRDD()\n","    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n","\n","    for x in aggregate_cols:\n","        if x.upper() in (name.upper() for name in df.columns):\n","            df_distinct_count = df.select(col(x)).distinct().count()\n","            df_distinct = spark.createDataFrame([[x, str(df_distinct_count)]],schema=schema)\n","            resultdf = resultdf.union(df_distinct)\n","\n","    return resultdf"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f790130e-446c-4594-88ee-796191cf7fde","showTitle":false,"title":""}},"outputs":[],"source":["def get_distribution_counts(spark, df, aggregate_cols):\n","    \"\"\" Get Distribution Counts for columns\n","    Args:\n","        spark (Spark): SparkSession object\n","        df (DataFrame): dataframe to perform null/empty analysis on\n","        aggregate_cols (List): list of columns that need to be considered for analysis\n","    Returns:\n","        Array: Array of objects with dataframes\n","    \"\"\"\n","    result = []\n","    for x in aggregate_cols:\n","        if x.upper() in (name.upper() for name in df.columns):\n","            result.append(df.groupby(col(x)).count().sort(col(\"count\").desc()))\n","    ###\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"368233ef-7a87-445d-ad74-4471b425c8b7","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def get_mismatch_perc(spark, df, data_quality_cols_regex):\n","    \"\"\" Get Mismatch Percentage for columns\n","    Args:\n","        spark (Spark): SparkSession object\n","        df (DataFrame): dataframe to perform null/empty analysis on\n","        data_quality_cols_regex (Dictionary): Dictionary of columns/regex-expression for data quality analysis\n","    Returns:\n","        DataFrame: DataFrame with data quality analysis\n","    \"\"\"\n","    schema = StructType([ \\\n","        StructField(\"Column\",StringType(),True), \\\n","        StructField(\"MismatchPercentage\",StringType(),True)\n","    ])\n","\n","    emptyRDD = spark.sparkContext.emptyRDD()\n","    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n","\n","\n","    for key, value in data_quality_cols_regex.items():\n","        if key.upper() in (name.upper() for name in df.columns):\n","            df_regex_not_like_count = df.select(col(key)).filter(~col(key).rlike(value)).count()\n","            df_regex_not_like = spark.createDataFrame([[key, str(df_regex_not_like_count*100.0/df.count()) + '%']],schema=schema)\n","            resultdf = resultdf.union(df_regex_not_like)\n","\n","    return resultdf"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2929545a-a25c-43cf-a0e2-3bc9f364840d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def generate_report(raw_df:pd.DataFrame,err_df:pd.DataFrame,destination_path:str)->str:\n","    \n","    \n","    \n","    wb = Workbook() # initializing an empty workbook\n","    ws = wb.active\n","    ws.title = 'Raw Data' # Sheet contains the raw data\n","    ws1 = wb.create_sheet(\"Data Overview\") #aggregation of the raw data\n","    ws2 = wb.create_sheet(\"DQ_Report\") # This is the worksheet where we will add the error df summary\n","    rows = dataframe_to_rows(raw_df, index = False, header = True)\n","\n","    for r_idx, row in enumerate(rows, 1):  \n","        for c_idx, value in enumerate(row, 1):\n","            ws.cell(row=r_idx, column=c_idx, value=value)\n","        \n","    pivot_df =raw_df\n","    pivot_df = pivot_df.replace(',','', regex=True)\n","    pivot_df['Week'] = pd.to_datetime(pivot_df['Week'], infer_datetime_format=True)\n","    pivot_df['Year-Week'] = pivot_df.Week.dt.strftime('%Y-w%V')\n","    pivot_df['Week'] = pivot_df['Week'].dt.date\n","    pivot_df['Net Spend (Local)'] = pivot_df['Net Spend (Local)'].replace(' -   ',np.nan, regex=True)\n","    pivot_df['Net Spend (Local)'] = pivot_df['Net Spend (Local)'].astype(float)\n","    pivot_df['Impressions'] = pivot_df['Impressions'].replace(' -   ',np.nan, regex=True).replace(\"\\ \", \"\",regex=True)\n","    pivot_df['Impressions'] = pivot_df['Impressions'].astype(float)\n","    pivot_df['Reach'] = pivot_df['Reach'].replace(' -   ',np.nan, regex=True)\n","    pivot_df['Reach'] = pivot_df['Reach'].replace(\"\\%\",\"\",regex=True).replace(\"\",np.nan,regex=True).astype(float)\n","    pivot_df = pivot_df.groupby(['Brand Generation','Product','Week','Year-Week']).agg({'Net Spend (Local)': nansum, 'Impressions': nansum, 'Reach': nansum}).reset_index() \n","  \n","    rows = dataframe_to_rows(pivot_df, index = False, header = True)\n","    for r_idx, row in enumerate(rows, 1): \n","        for c_idx, value in enumerate(row, 1):\n","            ws1.cell(row=r_idx, column=c_idx, value=value)\n","        \n","        \n","    rows = dataframe_to_rows(err_df, index = False, header = True)\n","    for r_idx, row in enumerate(rows, 1): \n","        for c_idx, value in enumerate(row, 1):\n","            ws2.cell(row=r_idx, column=c_idx, value=value)\n","        \n","        \n","    wb.save('/tmp/dq_report.xlsx')\n","    dest_path =  copyfile('/tmp/dq_report.xlsx',f'{destination_path}DQ_Report.xlsx')\n","\n","    return dest_path"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5669fd38-5afe-45ee-8d55-0b8210ada136","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["\n","\n","def add_ingestion_date(input_df):\n","    output_df = input_df.withColumn(\"ingestion_date\",current_timestamp())\n","    return output_df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8ebf73f3-cd47-46ad-ac4b-3097e93bc32d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def df_null_percentage(df = None)->pd.Series:\n","    train_missing = (1 - df.count()/len(df)) * 100\n","    return train_missing.sort_values(ascending = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9374d4c7-3bc5-4ced-a17e-bdef2d7d7001","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def get_null_dict(df:pd.DataFrame)->dict:\n","\n","    null_series = df_null_percentage(df)\n","    percentage_null_dict = dict()\n","\n","    for key in null_series.index:\n","        percentage_null_dict[key] = null_series[key]\n","    return percentage_null_dict "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f9db1c53-5546-4cf5-9b2c-17b23678c370","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def add_row_index_to_df(df:pd.DataFrame)->pd.DataFrame:\n","    df['ROW_ID'] = df.index + 1\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c8358a6d-5e7f-40b3-9dcc-c5e52d5a5bd0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def err_df_null_rule(null_check_columns:list, df:pd.DataFrame = None,threshold_per = 10)->pd.DataFrame:\n","    \n","    \"\"\"\n","    Function to return the DataFrame for DQ_Report Tab (Rows containing NULLs)\n","    Args:\n","        \n","        df (Pandas DataFrame): dataframe to perform null/empty analysis on\n","        null_check_columns (List): Columns to check for Null data\n","    Returns:\n","        DataFrame: DataFrame with Rows not passing the Null Rule threshold.\n","    \n","    \"\"\"\n","    percentage_null_dict = get_null_dict(df)\n","    df = add_row_index_to_df(df)\n","     \n","\n","    return_df = pd.DataFrame()\n","\n","   \n","    threshold_nulls = [col for col in null_check_columns if percentage_null_dict[col] > threshold_per]\n","\n","    try:\n","        for col in threshold_nulls:\n","            temp_df = pd.DataFrame()\n","            temp_df['rule_name'] = df[col].map(lambda x : f'null_value_for_the_{col}' if(pd.isnull(x)) else None)\n","\n","            temp_df = temp_df[temp_df['rule_name']!=None]\n","      \n","            temp_df['ROW_ID'] = temp_df.index+1\n","\n","            return_df = pd.concat([temp_df,return_df],axis=0)\n","\n","      \n","        return_df = return_df[return_df['rule_name'].notnull()]    \n","\n","        return pd.merge(return_df,df,on='ROW_ID')\n","\n","    except:\n","        \n","\n","        print(\"Passed Columns have passed null checks\")\n","        return return_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"42e60c5a-7672-48fb-93b1-7a75a4859b5a","showTitle":false,"title":""}},"outputs":[],"source":["def reconcilliation_tool(config_str, get_fuzzy_match = False, tol_val = 0.1):\n","    \"\"\"Checks data between dataframes for further reconcilliation and creates a report\n","\n","    Args:\n","        config_str: Config Path containing the complete details to run the module.\n","        get_fuzzy_match: Performs Fuzzy Matching using levenshtein distance and recommends closest matches.\n","    Returns:\n","        \n","    Raises:\n","        ValueError: if config is None or not parsed.\n","    \n","    TODO :\n","        1. Return Flag if the data does not match for adding to automated data quality pipelines based on a tolerance value\n","        2. Add Reconcile checks between numerical columns \n","        3. ignore_records config, so that we can ignore some rows from getting checked\n","\n","    \"\"\"\n","    \n","    try:\n","        \n","        config = yaml.load(config_str, Loader=yaml.FullLoader)\n","    except:\n","        raise ValueError('Not Able to Parse Config String')\n","    \n","    # Read files from config\n","    alpha_df = eval(config['datasources']['alpha_df'])\n","    beta_df = eval(config['datasources']['beta_df'])\n","    \n","    for key in config['reconcilation_check']:\n","        \n","        displayHTML( '<h1> Matching between {0} and {1} </h1>'.format(key, config['reconcilation_check'][key]))\n","        left_df =  alpha_df.select(key).drop_duplicates()\n","        right_df = beta_df.select(config['reconcilation_check'][key]).drop_duplicates()\n","        displayHTML( '<b> Number of Unique Values in {0} : {1}'.format(key, str(left_df.count())))\n","        displayHTML( '<b> Number of Unique Values in {0} : {1} </b>'.format(config['reconcilation_check'][key], str(right_df.count())))\n","                  \n","        DFJOIN  = left_df.join(beta_df, left_df[key] == right_df[config['reconcilation_check'][key]], \"outer\")\n","        DFJOINMERGE = DFJOIN.withColumn(\"_merge\", when(DFJOIN[key].isNull(), config['datasources']['beta_alias']).when(DFJOIN[config['reconcilation_check'][key]].isNull(), config['datasources']['alpha_alias']).otherwise(\"both\"))\n","        display(DFJOINMERGE.select(*[key]+[config['reconcilation_check'][key]] + ['_merge']))\n","        res = DFJOINMERGE.groupBy(\"_merge\").count().toPandas()\n","      \n","        try:\n","            displayHTML( '<b> Match Result Percentage : {0} </b>'.format(str(np.round((res[res['_merge'] == 'both']['count'].values[0]/res['count'].sum())*100, 2))))\n","            if np.round((res[res['_merge'] == 'both']['count'].values[0]/res['count'].sum())*100, 2) < (1-tol_val)*100:\n","                raise ValueError('Reconcilliation check failed due to not passing tolerance level')\n","        except:\n","            displayHTML( '<b> Match Result Percentage : {0} </b>'.format(str(0.00)))\n","            if 0 < (1-tol_val)*100:\n","                raise ValueError('Reconcilliation check failed due to not passing tolerance level')\n","      \n","        if get_fuzzy_match:\n","            displayHTML( '<b> Fuzzy Match between {0} and {1} </b>'.format(key, config['reconcilation_check'][key]))\n","            fuzzydf = left_df.join(right_df, levenshtein(lower(left_df[key]), lower(right_df[config['reconcilation_check'][key]])) < 3)\n","            display(fuzzydf)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a8ca01dc-fcae-431e-ae54-81348c55905a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def stage_data_to_s3():\n","    AWS_SECRET = ''\n","    AWS_KEY_ID =  ''\n","    import boto3\n","# Generate the boto3 client for interacting with S3\n","    s3 = boto3.resource('s3', region_name='us-east-1', \n","                        # Set up AWS credentials \n","                        aws_access_key_id=AWS_KEY_ID, \n","                         aws_secret_access_key=AWS_SECRET)\n","    \n","    AWS_BUCKET_NAME = \"demo-s3-stage\"\n","    SRC_FILE_PATH = f'/dbfs/FileStore/ROI_DQ/DQ_Report.xlsx'\n","    \n","    FILE_NAME = f'DQ_Report.xlsx'\n","    S3_DEST_PATH= f'{AWS_BUCKET_NAME}'+'/'+FILE_NAME\n","\n","\n","    bucket = s3.Bucket(AWS_BUCKET_NAME)\n","    bucket.upload_file(SRC_FILE_PATH, S3_DEST_PATH)\n","    print(f'file loaded to s3 path {S3_DEST_PATH}')\n","    "]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3530009255745714,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"common_functions","notebookOrigID":3530009255745695,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
